# -*- coding: utf-8 -*-
"""projectDuration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hqE3WL6kIwjFhpCYgSy8k5dUX2pwtIJE
"""

import numpy as np
import pandas as pd
from pandas.plotting import scatter_matrix
#from sklearn.preprocessing import Imputer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import sklearn.metrics as sm
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn import preprocessing
from sklearn.svm import SVR
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
# from nltk.tokenize import word_tokenize
import lightgbm as lgb
# from imblearn.combine import SMOTEENN
import re
import sys
#Read the data
data = pd.read_csv('D:\\Plus-Experience\\Plus-Experience\\ML\\Final_Upwork_Dataset.csv');

"""# **Preprocessing**"""

#Take the wanted features(columns)
del data['Job_URL']
del data['Posted_from']
del data['Category1_URL_search']
del data['Category2_URL_search']
del data['Category3_URL_search']
del data['Category4_URL_search']
del data['Category5_URL_search']
del data['Category6_URL_search']
del data['Category7_URL_search']
del data['Category8_URL_search']
del data['Category9_URL_search']
del data['Applicants_Num']
del data['Payment_Situation']
del data['Enterprise_Client']
del data['Freelancers_Num']
del data['Spent($)']
del data['Client_Country']
del data['Connects_Num']
del data['New_Connects_Num']
del data['Rating']
del data['Feedback_Num']
del data['Payment_type']
del data['Hourly_Rate']
del data['Start_rate']
del data['End_rate']
del data['EX_level_demand']
del data['Job_Cost']

#Fill NaN values with empty sapce to compine the columns
data["Category_1"].fillna("",inplace=True)
data["Category_2"].fillna("",inplace=True)
data["Category_3"].fillna("",inplace=True)
data["Category_4"].fillna("",inplace=True)
data["Category_5"].fillna("",inplace=True)
data["Category_6"].fillna("",inplace=True)
data["Category_7"].fillna("",inplace=True)
data["Category_8"].fillna("",inplace=True)
data["Category_9"].fillna("",inplace=True)
data["highlight"].fillna("",inplace=True)
data["Category_9"].fillna("",inplace=True)
data["Search_Keyword"].fillna("",inplace=True)

#Compine categories into 1 column
# remove space, underscore, and make it in lower case
def cleanData(x, operation):
  if operation == 'both':
    if isinstance(x,float) == False:
      return str.lower(x.replace(" ",""))
  elif operation == 'lower':
    if isinstance(x,float) == False:
      return str.lower(x)
  elif operation == 'space':
    if isinstance(x,float) == False:
      return x.replace(" ","")
  elif operation == 'underscore':
    if isinstance(x,float) == False:
      return x.replace("_","")
  else:
    print('Choose a correct method')

data["Category_1"] = data["Category_1"].apply(lambda x: cleanData(x, 'both'))
data["Category_2"] = data["Category_2"].apply(lambda x: cleanData(x, 'both'))
data["Category_3"] = data["Category_3"].apply(lambda x: cleanData(x, 'both'))
data["Category_4"] = data["Category_4"].apply(lambda x: cleanData(x, 'both'))
data["Category_5"] = data["Category_5"].apply(lambda x: cleanData(x, 'both'))
data["Category_6"] = data["Category_6"].apply(lambda x: cleanData(x, 'both'))
data["Category_7"] = data["Category_7"].apply(lambda x: cleanData(x, 'both'))
data["Category_8"] = data["Category_8"].apply(lambda x: cleanData(x, 'both'))
data["Category_9"] = data["Category_9"].apply(lambda x: cleanData(x, 'both'))

data["highlight"] = data["highlight"].apply(lambda x: cleanData(x, 'both'))

data["Description"] = data["Description"].apply(lambda x: cleanData(x, 'lower'))

data["Job Title"] = data["Job Title"].apply(lambda x: cleanData(x, 'lower'))

data["Search_Keyword"] = data["Search_Keyword"].apply(lambda x: cleanData(x, 'underscore'))
data["Search_Keyword"] = data["Search_Keyword"].apply(lambda x: cleanData(x, 'lower'))

#Creating the new column(skills column)
skills = data["Category_1"]+" "+data["Category_2"]+" "+data["Category_3"]+" "+data["Category_4"]+" "+data["Category_5"]+" "+data["Category_6"]+" "+data["Category_7"]+" "+data["Category_8"]+" "+data["Category_9"]+" "+data["highlight"]+" "+data["Search_Keyword"]

data['skills'] = skills

#Delete old columns
del data['Category_1']
del data['Category_2']
del data['Category_3']
del data['Category_4']
del data['Category_5']
del data['Category_6']
del data['Category_7']
del data['Category_8']
del data['Category_9']
del data['highlight']
del data['Search_Keyword']

# remove NaN values
imputer = SimpleImputer(missing_values=np.nan, strategy="most_frequent")
data.iloc[:,[1]] = imputer.fit_transform(data.iloc[:,[1]])

#Extract the duration time from Time_Limitation column
# convert the duration into binary numbers
def binaryConvert (x):
    if 'Less than 1 month' in x:
        return "0"
    elif '1 to 3 months' in x:
        return "1"
    elif '3 to 6 months' in x:
        return "10"
    elif 'More than 6 months' in x:
        return "11"

data["Time_Limitation"] = data["Time_Limitation"].apply(lambda x: binaryConvert(str(x)))


data["Description"].fillna("",inplace=True)
data["Job Title"].fillna("",inplace=True)

#Remove special character from 'Description' column
specialChar = ["$","&","+",",",":",";","=","?","@","#","|","'","<",">",".","^","*","(",")","%","!","\n","/"]
def specialCharacter (x, l):
    for i in l:
        if i in x:
            x = str(x.replace(i," "))
    return x

data["Description"] = data["Description"].apply(lambda x: specialCharacter(str(x),specialChar))

#Remove special character from 'Job Title' column
data["Job Title"] = data["Job Title"].apply(lambda x: specialCharacter(str(x),specialChar))

#Remove links from 'Description' column
def remove_urls(x):
    # Define a regex pattern to match URLs
    url_pattern = re.compile(r'https?://\S+|www\.\S+')

    # Use the sub() method to replace URLs with the specified replacement text
    text_without_urls = url_pattern.sub(" ",x)

    return text_without_urls

data["Description"] = data["Description"].apply(lambda x: remove_urls(str(x)))


"""## Encoding"""

#Encoding 'EX_level_demand' column

label_encoder = preprocessing.LabelEncoder()
data['Time_Limitation'] = label_encoder.fit_transform(data['Time_Limitation'])

#Encoding 'Job Title', 'Description', 'skills' columns

tfidf_vectorizer_desc = TfidfVectorizer(max_features=1500,stop_words='english')
descriptionTerms = tfidf_vectorizer_desc.fit_transform(data['Description'])

tfidf_vectorizer_skills = TfidfVectorizer(max_features=1500,stop_words='english')
skillsTerms = tfidf_vectorizer_skills.fit_transform(data['skills'])

tfidf_vectorizer_title= TfidfVectorizer(max_features=1500,stop_words='english')
titleTerms = tfidf_vectorizer_title.fit_transform(data['Job Title'])

"""# **Preparation for ML models**"""

X = np.hstack([titleTerms.toarray(), descriptionTerms.toarray(), skillsTerms.toarray()]) # Title + Description + skills

#Predicted feature
y = data['Time_Limitation']



"""# *lightGBM Classifier*"""

#Splitting the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Train the model
model = lgb.LGBMClassifier(verbose=-1)
# model.fit(X_train, y_train)
model.fit(X_train, y_train)

#Predict and Evaluate
y_pred = model.predict(X_test)

"""# Prediction function"""

def predict_project_duration(job_title, description, skills, model, label_encoder, tfidf_vectorizer_desc, tfidf_vectorizer_skills):
    # Create a new data frame with the given project details
    new_data = pd.DataFrame({
        'job_title': [job_title],
        'description': [description],
        'skills': [skills]
    })


    # TF-IDF Vectorization for Text Features
    description_features_new = tfidf_vectorizer_desc.transform(new_data['description'])
    skills_features_new = tfidf_vectorizer_skills.transform(new_data['skills'])
    title_features_new = tfidf_vectorizer_title.transform(new_data['skills'])

    # tfidf_vectorizer_title= TfidfVectorizer(max_features=1500,stop_words='english')
    # titleTerms = tfidf_vectorizer_title.fit_transform(data['Job Title'])

    # Convert sparse matrices to dense arrays
    job_title_encoded_dense_new = title_features_new.toarray()
    description_features_dense_new = description_features_new.toarray()
    skills_features_dense_new = skills_features_new.toarray()

    # Combine Features
    X_new = np.hstack([job_title_encoded_dense_new, description_features_dense_new, skills_features_dense_new])

    # Predict project duration
    prediction = model.predict(X_new)
    predicted_category = label_encoder.inverse_transform(prediction)
    if predicted_category[0] == "0":
       return "Less than 1 month "
    elif predicted_category[0] == "1":
       return "1 to 3 months "
    elif predicted_category[0] == "10":
       return "3 to 6 months "
    elif predicted_category[0] == "11":
       return "More than 6 months "

terms = sys.argv[1]
print(predict_project_duration(terms[0],terms[1],terms[2],model,label_encoder, tfidf_vectorizer_desc, tfidf_vectorizer_skills))
sys.stdout.flush()