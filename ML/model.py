# -*- coding: utf-8 -*-
"""aiTools.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZVFP5sukeGxJNBJiSySkqLNOZNrlONdq
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer  #TF-IDF
from sklearn.metrics.pairwise import linear_kernel  #Similar to "Cosine-similarity"
import json
import sys
aiTools = pd.read_csv("D:\\Plus-Experience\\Plus-Experience\\ML\\all_ai_tool.csv")

# accTools = pd.read_csv("futuretools-imgs.csv")  # more accurate dataset tools

# aiTools.head()

# accTools.head()

# accTools["tool"].nunique(dropna=True)

# Change column name
renamedaiTools = aiTools.rename(columns={"AI Tool Name": "tool"})

# renamedaiTools.head()

"""# #**1Try**"""

# # Remove unnecessary columns
# newaiTools = renamedaiTools[["tool","Tool Link","Useable For"]]

# newaccTools = accTools[["tool","tool_description"]]

# newaiTools.head()

# newaccTools.head()

# # Normalize the tool name (remove white space, convert to lower case)
# def cleanData(x, operation):
#     if operation == 'both':
#       if isinstance(x,float) == False:
#         return str.lower(x.replace(" ",""))
#     elif operation == 'lower':
#       if isinstance(x,float) == False:
#         return str.lower(x)
#     elif operation == 'space':
#       if isinstance(x,float) == False:
#         return x.replace(" ","")
#     else:
#       print('Choose a correct method')

# newaccTools["tool"] = newaccTools["tool"].apply(lambda x: cleanData(x, 'both'))

# newaiTools["tool"] = newaiTools["tool"].apply(lambda x: cleanData(x, 'both'))

# # both = 917, lower= 663, space= 445

# newaccTools.head()

# newaiTools.head()

# # Merging the datasets
# newTools = pd.merge(newaiTools, newaccTools, on='tool', how='right')

# # Make the "tool_description" in lower case
# newTools["tool_description"] = newTools["tool_description"].apply(lambda x: cleanData(x, 'lower'))

# newTools.head(10)

# print("Unique values count: ",newTools.tool.nunique())

# print("NaN values count: ",newTools["Useable For"].isna().sum())

# #Replace NaN with an empty string
# newTools = newTools.dropna()

# newTools = newTools.reset_index()

# newTools

# # Import TfIdfVectorizer from scikit-learn(TF-IDF)
# from sklearn.feature_extraction.text import TfidfVectorizer

# # Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'
# tfidf = TfidfVectorizer(stop_words='english')

# # Construct the required TF-IDF matrix by fitting and transforming the data
# tfidf_matrix = tfidf.fit_transform(newTools['tool_description'])

# # Output the shape of tfidf_matrix
# tfidf_matrix.shape

# print(tfidf_matrix)

# tfidf.get_feature_names_out()

# # Import linear_kernel
# from sklearn.metrics.pairwise import linear_kernel

# # Compute the cosine similarity matrix
# cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

# cosine_sim.shape

# #Construct a reverse map of indices and tools names
# indices = pd.Series(newTools.index, index=newTools['tool']).drop_duplicates()

# indices

# # print(accTools.category.unique(), "\n")
# # accTools.category.nunique(dropna = True)  # unique values number

# # Function that takes in movie title as input and outputs most similar movies
# def get_recommendations(title, cosine_sim=cosine_sim):
#     # Get the index of the movie that matches the title
#     #ERROR
#     idx = indices[title]
#     # print("idx:",idx)

#     # Get the pairwsie similarity scores of all movies with that movie
#     sim_scores = list(enumerate(cosine_sim[idx]))
#     print("sim_scores: ",sim_scores)

#     # Sort the movies based on the similarity scores
#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

#     # Get the scores of the 10 most similar movies
#     sim_scores = sim_scores[1:11]

#     # Get the movie indices
#     movie_indices = [i[0] for i in sim_scores]
#     # print("movie_indices:",movie_indices)

#     # Return the top 10 most similar movies
#     return newTools['tool'].iloc[movie_indices]

# get_recommendations("clipdrop")

"""# #**2Try**"""

# Remove unnecessary columns
newaiTools = renamedaiTools[["tool","Tool Link","Description","Useable For"]]

# newaiTools.head()

# Normalize the tool name (remove white space, convert to lower case)
def cleanData(x, operation):
    if operation == 'both':
      if isinstance(x,float) == False:
        return str.lower(x.replace(" ",""))
    elif operation == 'lower':
      if isinstance(x,float) == False:
        return str.lower(x)
    elif operation == 'space':
      if isinstance(x,float) == False:
        return x.replace(" ","")
    else:
      print('Choose a correct method')

newaiTools["tool"] = newaiTools["tool"].apply(lambda x: cleanData(x, 'both'))
newaiTools["Description"] = newaiTools["Description"].apply(lambda x: cleanData(x, 'lower'))

# newaiTools.head()

newTools = newaiTools
# newTools.head(10)

# print("Unique values count: ",newTools.tool.nunique())
# print("NaN values count: ",newTools["Tool Link"].isna().sum())

# Remove all links that dont start with https(not working links)
for x in newTools['Tool Link']:
  if 'http' not in x:
    index = newTools[newTools['Tool Link'] == x].index.values
    newTools.drop(index, axis=0, inplace = True)

newTools = newTools.reset_index()

# newTools

## Creating the 'Terms' column that has all the words in (description, useable for)

# Convert 'useable for' column to lowercase
newTools['Useable For'] = newTools['Useable For'].apply(lambda x: cleanData(x, 'lower'))

# Remove '/' and replace it with space
for r in newTools['Useable For']:
  if '/' in r:
    index = newTools[newTools['Useable For'] == r].index.values
    newTools.loc[index,['Useable For']] = r.replace('/',' ')

# Create the 'Terms' column
Terms = newTools['Description'] + newTools['Useable For']

# Add 'Terms' column to the dataset
newTools['Terms'] = Terms

# newTools.head()

# Import TfIdfVectorizer from scikit-learn(TF-IDF)
from sklearn.feature_extraction.text import TfidfVectorizer

# Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'
tfidf = TfidfVectorizer(stop_words='english')

# Construct the required TF-IDF matrix by fitting and transforming the data
tfidf_matrix = tfidf.fit_transform(newTools['Terms'])

# Output the shape of tfidf_matrix
# tokens = tfidf.get_feature_names_out()
# print(tfidf_matrix.shape)
# print(tfidf.get_feature_names_out()[5000:5020])
# print(tfidf.vocabulary_,'\n')
# print(tfidf.vocabulary_['learning'],'\n')
# print(tfidf_matrix[:,1])
# idf_: It returns the inverse document frequency vector of the document passed as a parameter.
# print(tfidf.idf_[0])

# Import linear_kernel
from sklearn.metrics.pairwise import linear_kernel

# Compute the cosine similarity matrix
cosine_sim = linear_kernel(tfidf_matrix)

cosine_sim.shape

cosine_sim[:,2846]

#Construct a reverse map of indices and tools names
indices = pd.Series(newTools.index, index=newTools['tool']).drop_duplicates()

indices

# # Function that takes in movie title as input and outputs most similar movies
# def get_recommendations(title, cosine_sim=cosine_sim):
#     # Get the index of the movie that matches the title
#     #ERROR
#     idx = indices[title]
#     # print("idx:",idx)

#     # Get the pairwsie similarity scores of all movies with that movie
#     sim_scores = list(enumerate(cosine_sim[idx]))

#     # Sort the movies based on the similarity scores
#     sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

#     # Get the scores of the 10 most similar movies
#     sim_scores = sim_scores[1:11]

#     # Get the movie indices
#     movie_indices = [i[0] for i in sim_scores]
#     print("movie_indices:",movie_indices)

#     # Return the top 10 most similar movies
#     return newTools[['tool','Tool Link']].iloc[movie_indices]

# recommendations = get_recommendations("midjourney")
# recommendations

# Function that takes in movie title as input and outputs most similar movies
def get_recommendations(term, cosine_sim=cosine_sim):
    # Get the index of the movie that matches the title
    idx = tfidf.vocabulary_[term]
    resutlDict = {}
    x = 0

    # Get the pairwsie similarity scores of all movies with that movie
    sim_scores = list(enumerate(tfidf_matrix[:,idx]))

    # Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar movies
    sim_scores = sim_scores[1:11]

    # Get the movie indices
    tool_indices = [i[0] for i in sim_scores]
    
    # Return the top 10 most similar movies
    result = newTools[['tool','Tool Link']].iloc[tool_indices]
    result = result.values

    for i in result:
      resutlDict[i[0]] = i[1];
      x = x+1;
    
    # print("\n\n\n################################ RESULT ################################")
    # print(f"\n{json.dumps(resutlDict, indent = 4)}\n\n\n")

    return json.dumps(resutlDict, indent = 4)

terms = sys.argv[1]
print(get_recommendations(terms))
sys.stdout.flush()